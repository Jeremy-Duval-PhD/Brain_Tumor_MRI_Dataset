{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b22002-ebf8-4d28-a7cf-7e6e8e0a9ccd",
   "metadata": {},
   "source": [
    "# Kaggle - Brain Tumor MRI Dataset\n",
    "\n",
    "You can find the dataset and some informations about on the [Kaggle page](https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset).\n",
    "\n",
    "For details on steps below, please see documentation in the *docs* directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54433d9c-7a56-46b6-a06c-d2a82d157d3f",
   "metadata": {},
   "source": [
    "## General part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba7b2a-b3ff-49d1-89c5-0372c1f67b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea948c38-4269-420b-b242-27bfe64bdca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # ignore GPU\n",
    "import tensorflow as tf\n",
    "print(\"TF utilise :\", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ea236-ba22-45a3-8049-99c1201a2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path management\n",
    "NOTEBOOK_PATH = Path(ipynbname.path())\n",
    "PROJECT_ROOT = Path(NOTEBOOK_PATH).resolve().parents[1]\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "PREP_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "CLASSES = [\"notumor\", \"glioma\", \"meningioma\", \"pituitary\"]\n",
    "\n",
    "# parameters\n",
    "IMG_SIZE = 260 \n",
    "SEED = 42\n",
    "SPLIT = [0.7, 0.30]  # train - validation -> test data are always in a separated directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748523c-5119-40bb-8a6e-d2917a3e8548",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001844e-ed3b-405a-a7c1-5da2a2af3356",
   "metadata": {},
   "source": [
    "### Split train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e8155b-4665-41a6-82ee-8a8e7cd8a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Récupérer toutes les images de `training/`\n",
    "train_dir = \"raw/training\"\n",
    "all_paths = []\n",
    "\n",
    "for class_name in os.listdir(train_dir):  # ex: glioma, meningioma, ...\n",
    "    folder = os.path.join(train_dir, class_name)\n",
    "    \n",
    "    for img_name in os.listdir(folder):\n",
    "        all_paths.append( (os.path.join(folder, img_name), class_name) )\n",
    "\n",
    "df_train = pd.DataFrame(all_paths, columns=[\"filepath\", \"label\"])\n",
    "\n",
    "# 2. Split stratifié → Train + Validation\n",
    "train_df, val_df = train_test_split(\n",
    "    df_train,\n",
    "    test_size=0.2,         # tu peux ajuster ici (ex: 0.1 ou 0.15)\n",
    "    stratify=df_train[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(train_df.shape, val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e9c47-76b0-479e-a2a9-aa1acc9ab24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f06786ae-38b0-42d8-8099-6feeb5e9b9ef",
   "metadata": {},
   "source": [
    "### Calculate clipping bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38824bce-dcad-4873-9691-d88c2f6bfae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b1965-a0b8-432f-8372-2769614030e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2578135f-048e-412b-9ab2-5db0c62d655d",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04269c87-4628-4da4-bbbe-6cb0df282f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA version has been change to be compatible with Tensorflow\n",
    "def crop_black_background_tf(img, thresh=10):\n",
    "    \"\"\"\n",
    "    Crop black background of an image.\n",
    "    img: tf.Tensor of shape [H, W, C] or [H, W], dtype=tf.float32\n",
    "    \"\"\"\n",
    "    # Convert to grayscale si nécessaire\n",
    "    if img.shape[-1] == 3:\n",
    "        img_gray = tf.image.rgb_to_grayscale(img)\n",
    "    else:\n",
    "        img_gray = img\n",
    "\n",
    "    # Create a pixel mask > thresh\n",
    "    mask = img_gray > thresh\n",
    "\n",
    "    # Find non-zero coordinates\n",
    "    coords = tf.where(mask[:, :, 0])  # coords pixels True\n",
    "\n",
    "    # Security if black image\n",
    "    def crop():\n",
    "        y0 = tf.reduce_min(coords[:, 0])\n",
    "        x0 = tf.reduce_min(coords[:, 1])\n",
    "        y1 = tf.reduce_max(coords[:, 0])\n",
    "        x1 = tf.reduce_max(coords[:, 1])\n",
    "        return img[y0:y1+1, x0:x1+1, :]\n",
    "    \n",
    "    def no_crop():\n",
    "        return img\n",
    "\n",
    "    return tf.cond(tf.shape(coords)[0] > 0, crop, no_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fed28b-aaac-4d33-802a-12a460163749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_tf(path, label, low_clip, high_clip, target_size=(260, 260)):\n",
    "    \"\"\"\n",
    "    Full TF preprocessing pipeline:\n",
    "    - read\n",
    "    - crop black background\n",
    "    - resize\n",
    "    - convert to float32\n",
    "    - z-score normalize (image-wise)\n",
    "    - clip using percentiles\n",
    "    \"\"\"\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels=3)  # [H,W,3]\n",
    "\n",
    "    # 2. Convert to float32 [0,1]\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    # 3. Crop black background\n",
    "    img = crop_black_background_tf(img, thresh=10)\n",
    "\n",
    "    # 4. Resize\n",
    "    img = tf.image.resize(img, target_size, method='area')\n",
    "\n",
    "    # 5. Z-score normalize per image\n",
    "    mean, variance = tf.nn.moments(img, axes=[0,1,2])\n",
    "    std = tf.sqrt(variance)\n",
    "    std = tf.maximum(std, 1e-6)  # sécurité pour éviter division par zéro\n",
    "    img = (img - mean) / std\n",
    "\n",
    "    # 6. Clip\n",
    "    img = tf.clip_by_value(img, low_clip, high_clip)\n",
    "\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308bd5fd-4983-4487-9e3b-13afc235eb66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b140b1-d6b0-4637-a353-a972973f1b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bt_env)",
   "language": "python",
   "name": "bt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
